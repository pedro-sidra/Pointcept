import clearml
import re
import requests
import clearml.datasets
import subprocess
from pathlib import Path
import os

import argparse

parser = argparse.ArgumentParser(description="Run script on remote clearml worker")

defaults = dict(
    project="mestre_pedro",
    task="pointcept - {command}",
    docker="pedrosidra0/pointcept:v2",
    docker_arguments="--shm-size=64000mb -e MKL_SERVICE_FORCE_INTEL=1 -v /home/freitas/.netrc:/root/.netrc",
    local=False,
    dataset_id="a38dbe849a49415f99196916d4f947c5",
    dataset_path="./data/scannet",
    pretrain="",
    pretrain_path="./pretrain.pth",
    output_model_location="exp/**/model/*.pth",
)

parser.add_argument(
    "command",
    help="command-line command to run on remote worker (use quotes to avoid confusing parameters with this script, e.g. 'bash remote.sh -g 2 -d scannet')",
)
parser.add_argument(
    "--project", help="name of clearml project", default=defaults["project"]
)
parser.add_argument("--task", help="name of clearml task", default=defaults["task"])
parser.add_argument(
    "--docker",
    help="complete tag of docker image to use on remote",
    default=defaults["docker"],
)
parser.add_argument(
    "--docker_arguments",
    help="additional arguments to use for `docker run` on remote",
    default=defaults["docker_arguments"],
)
parser.add_argument(
    "--local",
    action="store_true",
    help="don't run remotely good for testing locally",
    default=defaults["local"],
)
parser.add_argument(
    "--dataset_id",
    help="clearml id of dataset to download on remote",
    default=defaults["dataset_id"],
)
parser.add_argument(
    "--dataset_path",
    help="download the `dataset_id` to this path on remote",
    default=defaults["dataset_path"],
)
parser.add_argument(
    "--pretrain",
    help="URL http to pretrained model to use on remote ",
    default=defaults["pretrain"],
)
parser.add_argument(
    "--pretrain_path",
    help="local path to download pretrained weights to",
    default=defaults["pretrain_path"],
)
parser.add_argument(
    "--output_model_location",
    help="path to the outpt model generated by `command` to save as clearml artifact. Can use globl syntax, will take the first item returned by the glob",
    default=defaults["output_model_location"],
)

args = parser.parse_args()
for arg in args.__dict__:
    value = getattr(args, arg)
    if isinstance(value,str) and re.search("{\w+}", value):
        setattr(args, arg, value.format(**args.__dict__))

# setup clearml task and put into the queue
task: clearml.Task = clearml.Task.init(project_name=args.project, task_name=args.task)

task.connect(args)

task.set_base_docker(
    docker_image=args.docker,
    # TODO: fix this to be more generic than this workaround for credentials
    docker_arguments=args.docker_arguments,
)

# Execution stops here and continues on remote
if not args.local:
    task.execute_remotely(queue_name="default")

# Download dataset to `args.dataset_path`
data_path = clearml.datasets.Dataset.get(dataset_id=args.dataset_id).get_local_copy()
dataset_path = Path(args.dataset_path)
if not dataset_path.exists():
    print(f"Symlinking {data_path} to {dataset_path}")
    dataset_path.parent.mkdir(exist_ok=True, parents=True)
    os.symlink(src=data_path, dst=dataset_path)

# Download model to pretrain_path
pretrain_link = args.pretrain
pretrain_path = args.pretrain_path
if pretrain_link:
    r = requests.get(pretrain_link)
    open(pretrain_path, "wb").write(r.content)

# Run command on remote
try:
    subprocess.run(args.command.split(), stderr=subprocess.STDOUT)
except subprocess.CalledProcessError:
    print("WARNING: called command failed {args.command}")
finally:
    models = list(Path(".").glob(args.output_model_location))
    if len(models)>0:
        model_path = models[0]
        task.upload_artifact(name="model", artifact_object=str(model_path))
    else:
        print(f"no model found on {args.output_model_location}")
